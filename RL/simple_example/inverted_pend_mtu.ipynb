{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for inverted pendulum controlled with muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../Biodiffrl\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"\n",
    "\n",
    "# Optionally, force JAX to preallocate memory.\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"true\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# Setup environment variable for Nvidia GPU acceleration\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    \"--xla_gpu_triton_gemm_any=true\"\n",
    "    # '--xla_gpu_enable_async_collectives=true '\n",
    "    # '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    "    # '--xla_gpu_enable_highest_priority_async_stream=true '\n",
    "    # '--xla_cpu_multi_thread_eigen=true intra_op_parallelism_threads=32'\n",
    ")\n",
    "\n",
    "backend = 'gpu'\n",
    "# backend = 'METAL'\n",
    "# backend = 'cpu'\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "# os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_lazy_compilation=false\"\n",
    "# Enable compliation catch\n",
    "os.environ[\"JAX_COMPILATION_CACHE_DIR\"] = \"./jax_cache\"\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"./jax_cache\")\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", 0)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 2)\n",
    "# jax.config.update(\"jax_explain_cache_misses\", True)\n",
    "\n",
    "# Solve NaN on newer nvidia cards\n",
    "jax.config.update('jax_default_matmul_precision', \"high\")\n",
    "\n",
    "from jax.experimental.compilation_cache import compilation_cache as cc\n",
    "cc.set_cache_dir(\"./jax_cache\")\n",
    "# Debug Nan\n",
    "# jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "from jax import numpy as jp\n",
    "# More legible printing from numpy.\n",
    "jp.set_printoptions(precision=6, suppress=True, linewidth=100)\n",
    "\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "from mujoco.mjx._src import scan\n",
    "from mujoco.mjx._src import types\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=6, suppress=True, linewidth=100)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "device = jax.devices(backend=backend)[0]\n",
    "\n",
    "model_path = '/home/bugman/Currentwork/biomujoco_rl/RL/simple_example/inverted_pendulum_mtu.xml'\n",
    "# model_path = '/home/bugman/Currentwork/biomujoco_converter/converted/mjc/Gait2354/gait2354_cvt1_easy.xml'\n",
    "# model_path = \"/home/bugman/Downloads/Genesis-main/genesis/assets/xml/franka_emika_panda/panda.xml\"\n",
    "\n",
    "# Single step\n",
    "mjx_step = jax.jit(mjx.step, backend=backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "[0 2 1 3]\n",
      "[3 3 3 3]\n",
      "[2000. 2000.]\n",
      "[10. 10.]\n",
      "[0.9 0.9]\n",
      "[0.05236 0.05236]\n",
      "[1. 1.]\n",
      "-------Data--------\n",
      "qpos: [-0.  0.]\n",
      "mtu l: [2.1 2.1]\n",
      "tendon l: [0.913822 0.913822]\n",
      "fiber l : [1.187113 1.187113]\n",
      "Muscle Bce: [0.05 0.05]\n",
      "Muscle vm: [0.666667 0.666667]\n",
      "Fiber acc: [0.89215 0.89215]\n",
      "Fiber v: [0.000474 0.000474]\n",
      "Biomtu h: [0.047102 0.047102]\n",
      "[0. 0.]\n",
      "[0.047102 0.047102]\n",
      "[0.039689 0.039689]\n",
      "[0 0]\n",
      "[1 1]\n",
      "mtu act: [0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from mujoco.mjx._src.biomtu import acceleration_mtu\n",
    "\n",
    "mj_model = mujoco.MjModel.from_xml_path(model_path)\n",
    "mjx_model = mjx.put_model(mj_model,device=device)\n",
    "\n",
    "# Disable tendon\n",
    "# opt = mjx_model.opt.replace(disableflags = mjx_model.opt.disableflags |mujoco.mjtDisableBit.mjDSBL_PASSIVE)\n",
    "# mjx_model = mjx_model.replace(opt=opt)\n",
    "\n",
    "mjx_data = mjx.make_data(mjx_model)\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "\n",
    "# Load the Keyframe\n",
    "# mjx_data = mjx_data.replace(qpos = mj_model.key_qpos[0])\n",
    "# mj_data.qpos = mj_model.key_qpos[0]\n",
    "\n",
    "# Calculate equilibrum\n",
    "mjx_data = acceleration_mtu.calc_equilibrium(mjx_model, mjx_data)\n",
    "mjx_data = jax.jit(mjx_step)(mjx_model, mjx_data)\n",
    "\n",
    "init_mjx_data = mjx_data\n",
    "\n",
    "def print_all():\n",
    "    print(mjx_model.biomtu_adr)\n",
    "    print(mjx_model.mtu_wrap_objid)\n",
    "    print(mjx_model.mtu_wrap_type)\n",
    "    print(mjx_model.biomtu_fiso)\n",
    "    print(mjx_model.biomtu_vmax)\n",
    "    print(mjx_model.biomtu_ofl)\n",
    "    print(mjx_model.biomtu_opa)\n",
    "    print(mjx_model.biomtu_mass)\n",
    "    print(\"-------Data--------\")\n",
    "    print(\"qpos:\", mjx_data.qpos)\n",
    "    print(\"mtu l:\", mjx_data.biomtu.l)\n",
    "    print(\"tendon l:\", mjx_data.biomtu.tendon_l)\n",
    "    print(\"fiber l :\", mjx_data.biomtu.fiber_l)\n",
    "    print(\"Muscle Bce:\", mjx_data.biomtu.B_ce)\n",
    "    print(\"Muscle vm:\", mjx_data.biomtu.m)\n",
    "    print(\"Fiber acc:\", mjx_data.biomtu.fiber_acc)\n",
    "    print(\"Fiber v:\", mjx_data.biomtu.fiber_v)\n",
    "    print(\"Biomtu h:\", mjx_data.biomtu.h)\n",
    "    print(mjx_data.biomtu.v)\n",
    "    print(mjx_data.biomtu.h)  # The constant high of the muscle.\n",
    "    print(mjx_data.biomtu.pennation_angle)\n",
    "    print(mjx_data.biomtu.origin_body_id)\n",
    "    print(mjx_data.biomtu.insertion_body_id)\n",
    "    print(\"mtu act:\", mjx_data.biomtu.act)\n",
    "    # print(mjx_data.biomtu.j)\n",
    "    print(mjx_data.qfrc_biomtu)\n",
    "    # print(mj_model.key_time)\n",
    "    # print(mj_model.key_qpos)\n",
    "    # print(mj_model.key_qvel)\n",
    "\n",
    "print_all()\n",
    "\n",
    "# print(mjx_model.nbiomtu)\n",
    "# print(mjx_model.nq)\n",
    "# print(mjx_data.qpos)\n",
    "# print(mjx_data.qvel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exp\n",
    "\n",
    "# Multiple steps\n",
    "def step_fn(carry, _):\n",
    "    model, data= carry\n",
    "    new_data = mjx.step(model, data)\n",
    "    new_carry = (model, new_data)\n",
    "    return new_carry, _\n",
    "\n",
    "def multiple_steps(model, data):\n",
    "    init_carry = (model, data)\n",
    "    y, _ = jax.lax.scan(step_fn, init_carry, None, length=10)\n",
    "    new_data = y[1]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "jit_multiple_steps = jax.jit(multiple_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco.viewer\n",
    "import time\n",
    "\n",
    "# Debug Nan\n",
    "# jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "mj_model = mujoco.MjModel.from_xml_path(model_path)\n",
    "mjx_model = mjx.put_model(mj_model,device=device)\n",
    "\n",
    "# Disable tendon\n",
    "# opt = mjx_model.opt.replace(disableflags = mjx_model.opt.disableflags |mujoco.mjtDisableBit.mjDSBL_PASSIVE)\n",
    "# mjx_model = mjx_model.replace(opt=opt)\n",
    "\n",
    "mjx_data = mjx.make_data(mjx_model)\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "\n",
    "# Calculate equilibrum\n",
    "mjx_data = acceleration_mtu.calc_equilibrium(mjx_model, mjx_data)\n",
    "\n",
    "previous_frame_time = time.time()\n",
    "i = 0\n",
    "# key = jax.random.key(334)\n",
    "with mujoco.viewer.launch_passive(mj_model, mj_data) as viewer:\n",
    "    while viewer.is_running():\n",
    "        # Update mjx_data from mj_data. The mj_data was modified by the viewer\n",
    "        # mjx_data = mjx_data.replace(ctrl=mj_data.ctrl, xfrc_applied=mj_data.xfrc_applied)\n",
    "        # Use the nerual network to generate ctrl signal\n",
    "        \n",
    "        mjx_data = mjx_data.replace(xfrc_applied=jp.array(mj_data.xfrc_applied*10, dtype=jp.float32))\n",
    "        \n",
    "        # Generate key\n",
    "        # key = jax.random.split(key,1)[0]\n",
    "        # xfrc = jax.random.uniform(key,(mjx_model.nbody, 6), minval=-10, maxval=10)\n",
    "        # mjx_data = mjx_data.replace(xfrc_applied=xfrc)\n",
    "        mjx_data = mjx_data.replace(\n",
    "            qpos= jp.array(mj_data.qpos, dtype=jp.float32),\n",
    "            qvel= jp.array(mj_data.qvel, dtype=jp.float32),\n",
    "            time = jp.array(mj_data.time, dtype=jp.float32))\n",
    "        \n",
    "        # Update mjx_model from mj_model\n",
    "        mjx_model = mjx_model.tree_replace({\n",
    "            'opt.gravity': jp.array(mj_model.opt.gravity, dtype=jp.float32),\n",
    "            'opt.tolerance': jp.array(mj_model.opt.tolerance, dtype=jp.float32),\n",
    "            'opt.ls_tolerance': jp.array(mj_model.opt.ls_tolerance, dtype=jp.float32),\n",
    "            'opt.timestep': jp.array(mj_model.opt.timestep, dtype=jp.float32),\n",
    "        })\n",
    "        \n",
    "        \n",
    "        # Control Muscle\n",
    "        mjx_data = mjx_data.replace(biomtu = mjx_data.biomtu.replace(act = jp.array([0,0])))\n",
    "        \n",
    "        # mjx_data = mjx_step(mjx_model, mjx_data)\n",
    "        mjx_data = jit_multiple_steps(mjx_model, mjx_data)\n",
    "        # mjx_data, loss, exps = jit_nn_multi_steps(controller_params, mjx_model, mjx_data, key)\n",
    "        # mjx_data, key, act = jit_nn_mjx_one_step_no_random(controller_params, mjx_model, mjx_data, key)\n",
    "        \n",
    "        mjx.get_data_into(mj_data, mj_model, mjx_data)\n",
    "        \n",
    "        # Record the current time at the start of this frame\n",
    "        current_frame_time = time.time()\n",
    "    \n",
    "        # Calculate the difference in time from the last frame\n",
    "        time_between_frames = current_frame_time - previous_frame_time\n",
    "    \n",
    "        # Print the time between frames\n",
    "        print(f\"Time between frames: {time_between_frames} seconds\")\n",
    "        previous_frame_time = current_frame_time\n",
    "        \n",
    "        # print(\"ACT:\", mjx_data.biomtu.act)\n",
    "        print(mjx_data.qpos)\n",
    "        # print(mj_data.sen)  \n",
    "        # print(mjx_data.sensordata[3:6])\n",
    "        # print(mjx_data.biomtu.act)\n",
    "        # print(mjx_data.qfrc_inverse[6], mjx_data.qfrc_inverse[15] )\n",
    "        # print(mjx_data.qfrc_constraint[6], mjx_data.qfrc_constraint[15])\n",
    "        # print(len(mjx_data.qvel))\n",
    "        \n",
    "        \n",
    "        viewer.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([0.209172, 0.215768], dtype=float32), Array([0.20899 , 0.215497], dtype=float32), Array([-0.49779 , -0.102347], dtype=float32))\n",
      "[-1.48502]\n"
     ]
    }
   ],
   "source": [
    "import nn\n",
    "\n",
    "key = jax.random.key(6848)\n",
    "# Controller NN\n",
    "controller_nn = nn.Controller_NN(mjx_model.nq*2+mjx_model.nbiomtu*2, 2)     # There are two muscle so two output\n",
    "controller_params, key = controller_nn.init_parameters(key)\n",
    "controller = controller_nn.get_fn()\n",
    "\n",
    "# Critic NN\n",
    "critic_nn = nn.Critic_NN(mjx_model.nq*2 + mjx_model.nbiomtu*2 ,1)\n",
    "critic_params, key = critic_nn.init_parameters(key)\n",
    "criticer = critic_nn.get_fn()\n",
    "\n",
    "# Test the two neural networks\n",
    "print(controller(controller_params, jp.ones(mjx_model.nq*2+mjx_model.nbiomtu*2), key))\n",
    "print(criticer(critic_params, jp.ones(mjx_model.nq*2+mjx_model.nbiomtu*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# Since the environment resets with init_mjx_data, the muscles are already in the equilibrum condition.\n",
    "def reset(init_mjx_data, batch_size):\n",
    "    new_data = jax.tree.map(\n",
    "        partial(jp.repeat, repeats=batch_size, axis=0),\n",
    "        jax.tree.map(partial(jp.expand_dims, axis=0),init_mjx_data))\n",
    "    return new_data\n",
    "\n",
    "def random_init(data, model, rng: jax.Array):\n",
    "    nbiomtu = model.nq\n",
    "    init_qpos = data.qpos\n",
    "    init_qvel = data.qvel\n",
    "    new_rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "    # Qpos_1 is the vertical position\n",
    "    random_qpos = init_qpos + jax.random.uniform(rng1, [nbiomtu], minval=jp.array(-1.0, dtype=jp.float32), maxval=jp.array(1.0, dtype=jp.float32))*0.01\n",
    "    random_qvel = init_qvel + jax.random.uniform(rng2, [nbiomtu], minval=jp.array(-1.0, dtype=jp.float32), maxval=jp.array(1.0, dtype=jp.float32))*0.01\n",
    "    newdata = data.replace(qpos=random_qpos)\n",
    "    newdata = newdata.replace(qvel=random_qvel)\n",
    "    newdata = mjx.forward(mjx_model, newdata)\n",
    "    # print('data:',data.qpos, data.qvel)\n",
    "    # Calculate equilibrum\n",
    "    # newdata = acceleration_mtu.calc_equilibrium(mjx_model, newdata)\n",
    "    # newdata = mjx_step(mjx_model, newdata)\n",
    "    return newdata, new_rng\n",
    "\n",
    "vrandom_init = jax.jit(jax.vmap(random_init, in_axes=(None, None, 0), out_axes=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exp\n",
    "\n",
    "def step_fn(carry, _):\n",
    "    data, model = carry\n",
    "    new_data = mjx.step(model, data)\n",
    "    new_carry = (new_data, model)\n",
    "    return new_carry, _\n",
    "\n",
    "def multiple_steps(model, data):\n",
    "    init_carry = (data, model)\n",
    "    y, _ = jax.lax.scan(step_fn, init_carry, None, length=10)\n",
    "    new_data = y[0]\n",
    "    return new_data\n",
    "\n",
    "def nn_mjx_one_step(nn_params, model, data, key):\n",
    "    states = jp.concatenate([data.qpos, data.qvel, data.biomtu.fiber_l, data.biomtu.fiber_v])\n",
    "    act, mean, std = controller(nn_params, states, key)\n",
    "    # Generate the next key\n",
    "    new_key = jax.random.split(key,1)[0]\n",
    "    data = data.replace(biomtu = data.biomtu.replace(act = act))\n",
    "    new_data = mjx.step(model, data)\n",
    "    return new_data, new_key, act\n",
    "\n",
    "def nn_mjx_perturbe_one_step(nn_params, model, data, key):\n",
    "    states = jp.concatenate([data.qpos, data.qvel, data.biomtu.fiber_l, data.biomtu.fiber_v])\n",
    "    act, mean, std = controller(nn_params, states, key)\n",
    "    # Generate the next key\n",
    "    new_key = jax.random.split(key,1)[0]\n",
    "    data = data.replace(biomtu = data.biomtu.replace(act = act))\n",
    "    xfrc = jax.random.normal(key,(mjx_model.nbody, 6))*1.0\n",
    "    data = data.replace(xfrc_applied=xfrc)\n",
    "    new_data = mjx.step(model, data)\n",
    "    return new_data, new_key, act\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def jit_nn_mjx_one_step_no_random(nn_params, model, data, key):\n",
    "    states = jp.concatenate([data.qpos, data.qvel, data.biomtu.fiber_l, data.biomtu.fiber_v])\n",
    "    act, mean, std = controller(nn_params, states, key)\n",
    "    # Generate the next key\n",
    "    new_key = jax.random.split(key,1)[0]\n",
    "    data = data.replace(biomtu = data.biomtu.replace(act = act))\n",
    "    new_data = mjx.step(model, data)\n",
    "    return new_data, new_key, mean\n",
    "\n",
    "def nn_step_fn(carry, _):\n",
    "    nn_params, model, data, key = carry\n",
    "    new_data, new_key, act = nn_mjx_one_step(nn_params, model, data, key)\n",
    "    # new_data, new_key, act = nn_mjx_perturbe_one_step(nn_params, model, data, key)\n",
    "    new_carry = (nn_params, model, new_data, new_key)\n",
    "    # Calculate reward\n",
    "    states = jp.concat([data.qpos, data.qvel, data.biomtu.fiber_l, data.biomtu.fiber_v])\n",
    "    next_states = jp.concat([new_data.qpos, new_data.qvel, new_data.biomtu.fiber_l, new_data.biomtu.fiber_v])\n",
    "    action = act\n",
    "    reward = -new_data.qpos[1]**2\n",
    "    done = jp.abs(new_data.qpos[1])>0.8\n",
    "    experience = exp.experience(states, next_states, action, reward, done)\n",
    "    \n",
    "    return new_carry, experience\n",
    "\n",
    "def decay_sum_scan(x, decay):\n",
    "    def f(sxtm1, xt):\n",
    "        b = xt + decay * sxtm1\n",
    "        return b, b\n",
    "    return jax.lax.scan(f, jp.zeros(x.shape[1:]), x)[1]\n",
    "\n",
    "@jax.jit\n",
    "def jit_nn_multi_steps(nn_params, model, data, key):\n",
    "    # Also deal with the done in the experience pool\n",
    "    \n",
    "    repeat_length = 32\n",
    "    init_carry = (nn_params, model, data, key)\n",
    "    y, experience = jax.lax.scan(nn_step_fn, init_carry, None, length=repeat_length)\n",
    "    new_data = y[2]\n",
    "    new_key = y[3]\n",
    "    \n",
    "    return new_data, new_key, experience\n",
    "\n",
    "# @jax.jit\n",
    "def v_nn_multi_steps(nn_params, model, data, keys):\n",
    "    return jax.vmap(jit_nn_multi_steps, in_axes=(None, None, 0, 0))(nn_params, model, data, keys)\n",
    "\n",
    "jit_v_nn_multi_steps = jax.jit(v_nn_multi_steps)\n",
    "\n",
    "# This function generate\n",
    "@jax.jit\n",
    "def jit_vv_nn_multi_steps(nn_params, model, data, key):\n",
    "    return jax.vmap(jit_v_nn_multi_steps, in_axes=(None, None, None, 1))(nn_params, model, data, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "import exp\n",
    "\n",
    "critic_batch_size =500\n",
    "controller_batch_size = 100\n",
    "key = jax.random.key(2024)\n",
    "keys = jax.random.split(key, controller_batch_size)\n",
    "\n",
    "memory_settings = exp.memory_settings(critic_batch_size*5, mjx_model.nq*2+mjx_model.nbiomtu*2, mjx_model.nbiomtu, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pool = None\n",
    "datas = jax.jit(reset,static_argnames=\"batch_size\")(init_mjx_data, controller_batch_size)\n",
    "init_data_batch = datas\n",
    "for i in range(10):\n",
    "    datas, keys, exps = jit_v_nn_multi_steps(controller_params, mjx_model, datas, keys)\n",
    "    # print(datas.qvel.shape, datas.ten_J.shape)\n",
    "    exp_pool = exp.memory.add_exp(memory_settings, exp_pool, exps)\n",
    "    \n",
    "print(exp_pool.states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "v_criticer = jax.vmap(criticer,in_axes=(None, 0))\n",
    "jit_v_criticer = jax.jit(v_criticer)\n",
    "\n",
    "def critic_loss(params, batch):\n",
    "    discount = 0.99\n",
    "    states = batch.states\n",
    "    next_states = batch.next_states\n",
    "    actions = batch.actions\n",
    "    rewards = batch.rewards\n",
    "    \n",
    "    critic_score = v_criticer(params, states)\n",
    "    target = rewards + discount* jax.lax.stop_gradient(v_criticer(params, next_states))\n",
    "    # target = rewards + discount* (v_criticer(params, next_states))\n",
    "    \n",
    "    loss = optax.l2_loss(critic_score, target)\n",
    "    loss = jp.mean(loss)\n",
    "    return loss\n",
    "\n",
    "sample_batch = exp.memory.sample(exp_pool, critic_batch_size, key)\n",
    "critic_loss_g_value_lower= jax.jit(jax.value_and_grad(critic_loss)).lower(critic_params, sample_batch)\n",
    "\n",
    "jit_critic_loss_g_value = critic_loss_g_value_lower.compile()\n",
    "a=jit_critic_loss_g_value.cost_analysis()[0]['flops']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_sum_scan(x, decay):\n",
    "    def f(sxtm1, xt):\n",
    "        b = xt + decay * sxtm1\n",
    "        return b, b\n",
    "    return jax.lax.scan(f, jp.zeros(x.shape[1:]), x)[1]\n",
    "\n",
    "def controller_loss_and_experience(controller_params, critic_params, batch, batch_size, mjx_model, init_data_batch, keys):\n",
    "    # Generate data for simulation\n",
    "    nq = mjx_model.nq\n",
    "    nmtu = mjx_model.nbiomtu\n",
    "    \n",
    "    # Deal with the done state, reset the done state with init state\n",
    "    # exp_data_batch = init_data_batch.replace(\n",
    "    #     qpos = batch.states[:,0:nq], \n",
    "    #     qvel = batch.states[:,nq:nq*2],\n",
    "    #     biomtu = init_data_batch.biomtu.replace(\n",
    "    #         fiber_l = batch.states[nq*2, nq*2+nmtu],\n",
    "    #         fiber_v = batch.states[nq*2+nmtu, nq*2+nmtu*2]\n",
    "    #     ))\n",
    "    \n",
    "    qpos = jp.where(batch.dones==1, init_data_batch.qpos, batch.states[:,0:nq])\n",
    "    qvel = jp.where(batch.dones==1, init_data_batch.qvel, batch.states[:,nq:nq*2])\n",
    "    fiber_l = jp.where(batch.dones==1, init_data_batch.biomtu.fiber_l, batch.states[:,nq*2 : nq*2+nmtu])\n",
    "    fiber_v = jp.where(batch.dones==1, init_data_batch.biomtu.fiber_v, batch.states[:,nq*2+nmtu : nq*2+nmtu*2])\n",
    "    \n",
    "    # in_data = batch.dones, init_data_batch, exp_data_batch)\n",
    "    \n",
    "    in_data = jax.lax.stop_gradient(init_data_batch.replace(\n",
    "        qpos = qpos,\n",
    "        qvel = qvel,\n",
    "        biomtu = init_data_batch.biomtu.replace(\n",
    "            fiber_l = fiber_l,\n",
    "            fiber_v = fiber_v)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    out_data, keys, new_exps = v_nn_multi_steps(controller_params, mjx_model, in_data, keys)\n",
    "    # out_states = new_exps.next_states\n",
    "    # jax.debug.print(\"out_states shape{0}\", out_states.shape)\n",
    "    out_states = jp.concat([out_data.qpos, out_data.qvel, out_data.biomtu.fiber_l, out_data.biomtu.fiber_v, out_data.sensordata],axis=1)\n",
    "    critic_score = jp.squeeze(v_criticer(critic_params, out_states))\n",
    "    decay_M = 0.99 ** jp.arange(new_exps.rewards.shape[1]-1)\n",
    "    # print(new_exps.rewards.shape)\n",
    "    loss = -jp.mean(decay_M@new_exps.rewards.T[:-1,:])\n",
    "    # loss = -jp.mean(decay_M@new_exps.rewards.T[:-1,:]) - jp.mean(critic_score)\n",
    "    # loss = -jp.mean(critic_score)\n",
    "    return loss, new_exps\n",
    "\n",
    "# The function calculating the loss of the controller and also generate experiences\n",
    "g_loss_experience = jax.value_and_grad(controller_loss_and_experience, has_aux=True)\n",
    "\n",
    "controller_keys = jax.random.split(key, controller_batch_size)\n",
    "sample_batch = exp.memory.sample(exp_pool, controller_batch_size, key)\n",
    "\n",
    "print(\"lowering\")\n",
    "g_loss_experience_lower = jax.jit(g_loss_experience, static_argnames=[\"batch_size\"]).lower(\n",
    "    controller_params, \n",
    "    critic_params, \n",
    "    sample_batch, \n",
    "    controller_batch_size, \n",
    "    mjx_model, \n",
    "    init_data_batch, \n",
    "    controller_keys)\n",
    "\n",
    "print(\"compiling\")\n",
    "jit_g_loss_experience = g_loss_experience_lower.compile()\n",
    "\n",
    "b = jit_g_loss_experience.cost_analysis()[0]['flops']\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch.states.shape)\n",
    "\n",
    "b/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the two Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'linear1': {'kernel': Array([[-0.521868, -0.223058, -0.413956, ...,  0.708522,  0.49591 ,  0.465597],\n",
      "       [-0.092645,  0.019458, -0.3183  , ..., -0.489226, -0.267101,  0.305509],\n",
      "       [ 0.203467,  0.342604, -0.404305, ...,  0.114232,  0.61902 ,  0.532535],\n",
      "       ...,\n",
      "       [-0.289248,  0.179362, -0.328398, ...,  0.45507 , -0.071455, -0.149657],\n",
      "       [-0.321705,  0.214219,  0.25602 , ...,  0.120517, -0.50708 ,  0.535591],\n",
      "       [-0.527762, -0.095684, -0.075469, ..., -0.669847, -0.029087,  0.397224]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'LayerNorm_0': {'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1.], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'linear2': {'kernel': Array([[-0.073694,  0.009181,  0.045982, ...,  0.039568,  0.027536, -0.013754],\n",
      "       [-0.071544, -0.037184,  0.017876, ..., -0.023627,  0.014415, -0.042357],\n",
      "       [-0.014431,  0.051034, -0.046835, ..., -0.069068,  0.068253, -0.060331],\n",
      "       ...,\n",
      "       [ 0.053446,  0.05546 ,  0.070255, ...,  0.011255,  0.029941,  0.045899],\n",
      "       [ 0.023106,  0.060864, -0.044464, ..., -0.050767,  0.00243 ,  0.014549],\n",
      "       [ 0.066376, -0.014604, -0.03116 , ..., -0.014314, -0.008387,  0.026064]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)}, 'LayerNorm_1': {'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)}, 'linear3': {'kernel': Array([[-0.016621,  0.039537, -0.004427, ..., -0.010499, -0.020484, -0.041719],\n",
      "       [ 0.02126 ,  0.08358 ,  0.115038, ..., -0.077357,  0.034721, -0.102724],\n",
      "       [ 0.029812,  0.007478, -0.070781, ..., -0.028859,  0.135548,  0.086779],\n",
      "       ...,\n",
      "       [-0.013771, -0.046861, -0.035625, ...,  0.070152, -0.039417, -0.037199],\n",
      "       [-0.056018,  0.0877  ,  0.01251 , ..., -0.033182, -0.100485,  0.109299],\n",
      "       [-0.064395,  0.015824, -0.071735, ...,  0.066457, -0.014059, -0.115586]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'LayerNorm_2': {'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1.], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'linear4': {'kernel': Array([[ 0.072998, -0.058461,  0.065538, ...,  0.079767, -0.057836,  0.001434],\n",
      "       [ 0.05358 ,  0.026924,  0.025951, ...,  0.048472,  0.032456,  0.026008],\n",
      "       [ 0.087478, -0.05976 , -0.003246, ..., -0.034884, -0.005957, -0.009   ],\n",
      "       ...,\n",
      "       [ 0.053219,  0.03609 , -0.049446, ...,  0.000659,  0.007695, -0.000556],\n",
      "       [ 0.083192, -0.02954 ,  0.052841, ...,  0.013553,  0.078073,  0.016749],\n",
      "       [-0.063258,  0.001417, -0.082382, ...,  0.029866, -0.032242, -0.042735]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'LayerNorm_3': {'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1.], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'linear5': {'kernel': Array([[ 0.007107, -0.063807,  0.068391,  0.037024],\n",
      "       [-0.006915,  0.034831, -0.006558, -0.035724],\n",
      "       [-0.047857,  0.087786,  0.027304, -0.032104],\n",
      "       ...,\n",
      "       [ 0.062702, -0.007307,  0.016614,  0.056013],\n",
      "       [-0.081818,  0.058283,  0.011546,  0.020913],\n",
      "       [-0.000983,  0.073723,  0.046195, -0.013178]], dtype=float32), 'bias': Array([0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "key = jax.random.key(3454365435)\n",
    "\n",
    "critic_params, key = critic_nn.init_parameters(key)\n",
    "critic_tx = optax.apply_if_finite(optax.adam(learning_rate=1e-6), max_consecutive_errors=10)\n",
    "# critic_tx = optax.apply_if_finite(optax.sgd(learning_rate=1e-5), max_consecutive_errors=50)\n",
    "critic_opt_state = critic_tx.init(critic_params)\n",
    "jit_critic_tx_update = jax.jit(critic_tx.update)\n",
    "\n",
    "\n",
    "controller_params, key = controller_nn.init_parameters(key)\n",
    "controller_tx = optax.apply_if_finite(optax.adam(learning_rate=1e-5), max_consecutive_errors=50)\n",
    "# controller_tx = optax.apply_if_finite(optax.sgd(learning_rate=1e-2), max_consecutive_errors=50)\n",
    "# controller_tx = optax.sgd(learning_rate=1e-6)\n",
    "controller_opt_state = controller_tx.init(controller_params)\n",
    "jit_controller_tx_update = jax.jit(controller_tx.update)\n",
    "\n",
    "jit_apply_update = jax.jit(optax.apply_updates)\n",
    "\n",
    "\n",
    "jit_sample = jax.jit(exp.memory.sample, static_argnames=\"batch_size\")\n",
    "jit_add_exp = jax.jit(exp.memory.add_exp, static_argnames=\"settings\")\n",
    "\n",
    "print(controller_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data(key,mjx_model):\n",
    "    mjx_data = mjx.make_data(mjx_model)\n",
    "    keys = key = jax.random.split(key,4)\n",
    "    random_qpos = jax.random.uniform(keys[0], [2], minval=jp.array(-1.0, dtype=jp.float32), maxval=jp.array(1.0, dtype=jp.float32))\n",
    "    random_qvel = jax.random.uniform(keys[1], [2], minval=jp.array(-1.0, dtype=jp.float32), maxval=jp.array(1.0, dtype=jp.float32))*0.1\n",
    "    \n",
    "    random_qpos = random_qpos.at[0].set(random_qpos[0]*0.1)\n",
    "    random_qpos = random_qpos.at[1].set(random_qpos[1]*0.3)\n",
    "    \n",
    "    mjx_data = mjx_data.replace(qpos = random_qpos, qvel = random_qvel)\n",
    "\n",
    "    # Calculate equilibrum\n",
    "    mjx_data = acceleration_mtu.calc_equilibrium(mjx_model, mjx_data)\n",
    "    \n",
    "    return mjx_data\n",
    "\n",
    "jit_v_init_data = jax.jit(jax.vmap(init_data, in_axes=(0,None)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[[-0.047606  0.245926]\n",
      " [ 0.018174 -0.010012]\n",
      " [ 0.038285 -0.25566 ]\n",
      " [ 0.092525 -0.181915]\n",
      " [-0.03747   0.190355]\n",
      " [-0.07585  -0.165863]\n",
      " [ 0.0894   -0.024966]\n",
      " [ 0.08695   0.048732]\n",
      " [ 0.095481  0.293775]\n",
      " [-0.011247  0.266156]\n",
      " [-0.032044  0.207545]\n",
      " [-0.068613  0.036631]\n",
      " [ 0.088801 -0.069611]\n",
      " [-0.015412 -0.071564]\n",
      " [ 0.077661  0.111434]\n",
      " [ 0.050826  0.062921]\n",
      " [ 0.081484 -0.281168]\n",
      " [-0.014611  0.201475]\n",
      " [-0.092305 -0.251995]\n",
      " [ 0.058024  0.145313]\n",
      " [ 0.01638   0.071555]\n",
      " [ 0.058805 -0.079044]\n",
      " [-0.035041 -0.266214]\n",
      " [-0.083401  0.121915]\n",
      " [ 0.043826 -0.028575]\n",
      " [-0.094484  0.001002]\n",
      " [-0.022824  0.298112]\n",
      " [ 0.032444  0.229646]\n",
      " [-0.051935  0.127979]\n",
      " [-0.046064 -0.223181]\n",
      " [ 0.017213  0.165302]\n",
      " [ 0.074713 -0.06714 ]\n",
      " [-0.070857 -0.133987]\n",
      " [ 0.019137  0.071649]\n",
      " [ 0.049287 -0.042739]\n",
      " [-0.003312 -0.189575]\n",
      " [-0.038898 -0.136091]\n",
      " [ 0.087671  0.187513]\n",
      " [-0.064839  0.142492]\n",
      " [-0.070903  0.036709]\n",
      " [-0.005474 -0.154675]\n",
      " [-0.050284 -0.03171 ]\n",
      " [-0.098367 -0.291935]\n",
      " [ 0.029427 -0.255281]\n",
      " [ 0.062445 -0.286804]\n",
      " [-0.004049  0.18323 ]\n",
      " [-0.022367  0.018165]\n",
      " [ 0.028268 -0.213547]\n",
      " [ 0.005181 -0.272412]\n",
      " [ 0.045781  0.208535]\n",
      " [ 0.056946  0.02853 ]\n",
      " [-0.017473 -0.119499]\n",
      " [-0.02953   0.203291]\n",
      " [-0.030848 -0.059348]\n",
      " [ 0.049663  0.059356]\n",
      " [ 0.020385 -0.152632]\n",
      " [ 0.042002  0.005427]\n",
      " [ 0.025607 -0.189138]\n",
      " [-0.048722 -0.184624]\n",
      " [ 0.018339 -0.102807]\n",
      " [ 0.023626  0.109321]\n",
      " [-0.05664   0.130153]\n",
      " [ 0.073588  0.124838]\n",
      " [-0.041377 -0.047105]\n",
      " [ 0.069824 -0.067898]\n",
      " [-0.032897 -0.164428]\n",
      " [ 0.033189 -0.257997]\n",
      " [-0.010299 -0.011924]\n",
      " [ 0.050103 -0.130676]\n",
      " [-0.004715  0.159602]\n",
      " [ 0.009836  0.099363]\n",
      " [-0.080337  0.195128]\n",
      " [ 0.064058  0.037128]\n",
      " [-0.0623    0.22782 ]\n",
      " [ 0.045545  0.061702]\n",
      " [ 0.005826  0.209101]\n",
      " [-0.06851  -0.209964]\n",
      " [ 0.097907  0.240409]\n",
      " [-0.078107  0.160939]\n",
      " [-0.039389  0.15832 ]\n",
      " [-0.099751  0.136332]\n",
      " [-0.06497  -0.060337]\n",
      " [ 0.025005 -0.216576]\n",
      " [ 0.003531 -0.246249]\n",
      " [-0.033012  0.280651]\n",
      " [-0.049721 -0.201502]\n",
      " [-0.019411 -0.147977]\n",
      " [-0.040586 -0.15478 ]\n",
      " [ 0.009945  0.058002]\n",
      " [-0.095518 -0.027076]\n",
      " [ 0.061943 -0.219038]\n",
      " [-0.01192   0.111753]\n",
      " [ 0.078635  0.260408]\n",
      " [-0.092506  0.125008]\n",
      " [-0.0633    0.087504]\n",
      " [ 0.094475 -0.068181]\n",
      " [ 0.046994 -0.081845]\n",
      " [-0.039339  0.053962]\n",
      " [-0.008113 -0.198556]\n",
      " [-0.065385 -0.012974]]\n",
      "___________\n",
      "[[ 0.021922  0.01199 ]\n",
      " [ 0.068937 -0.054631]\n",
      " [-0.001544  0.098209]\n",
      " [-0.005617  0.097124]\n",
      " [-0.095352 -0.074855]\n",
      " [-0.083214  0.012655]\n",
      " [-0.068234 -0.096778]\n",
      " [ 0.06837  -0.066862]\n",
      " [ 0.06073  -0.082893]\n",
      " [-0.070707 -0.039087]\n",
      " [-0.031876  0.059743]\n",
      " [ 0.099772 -0.030084]\n",
      " [ 0.087542 -0.062077]\n",
      " [-0.047585  0.019771]\n",
      " [ 0.033748  0.035975]\n",
      " [-0.039219 -0.029229]\n",
      " [-0.033441  0.025516]\n",
      " [-0.020381  0.091936]\n",
      " [ 0.027283  0.005152]\n",
      " [-0.064129 -0.015838]\n",
      " [-0.048287 -0.079206]\n",
      " [-0.027981 -0.07317 ]\n",
      " [ 0.086399 -0.049176]\n",
      " [-0.007529  0.072809]\n",
      " [ 0.001225  0.09942 ]\n",
      " [-0.048216 -0.035431]\n",
      " [-0.070009  0.080215]\n",
      " [-0.029385  0.004538]\n",
      " [ 0.096344  0.056661]\n",
      " [ 0.049047  0.071853]\n",
      " [ 0.042987 -0.058719]\n",
      " [-0.077509 -0.070356]\n",
      " [ 0.032046  0.082243]\n",
      " [-0.063725  0.010984]\n",
      " [ 0.076521 -0.029618]\n",
      " [ 0.041759 -0.035915]\n",
      " [-0.083211  0.023857]\n",
      " [ 0.009945 -0.080633]\n",
      " [-0.038338 -0.034533]\n",
      " [-0.060273  0.067625]\n",
      " [ 0.04462  -0.012996]\n",
      " [-0.075883 -0.069826]\n",
      " [ 0.058114  0.005207]\n",
      " [ 0.033495  0.059199]\n",
      " [-0.069209  0.023306]\n",
      " [-0.084225  0.034042]\n",
      " [ 0.011934  0.073128]\n",
      " [-0.098847  0.007584]\n",
      " [ 0.003711 -0.08845 ]\n",
      " [ 0.020083 -0.096268]\n",
      " [-0.066744 -0.02375 ]\n",
      " [-0.073963 -0.023151]\n",
      " [ 0.069385  0.097672]\n",
      " [ 0.037925 -0.069527]\n",
      " [ 0.045407  0.089244]\n",
      " [-0.066873 -0.050633]\n",
      " [-0.081712 -0.081005]\n",
      " [ 0.012358 -0.013653]\n",
      " [-0.026727 -0.094865]\n",
      " [ 0.001759  0.061494]\n",
      " [-0.083385  0.010833]\n",
      " [ 0.075418  0.026396]\n",
      " [-0.049771  0.063913]\n",
      " [ 0.048775  0.038452]\n",
      " [-0.065879  0.003349]\n",
      " [-0.063499  0.05427 ]\n",
      " [ 0.040526 -0.003825]\n",
      " [ 0.019847 -0.068326]\n",
      " [ 0.076453  0.000858]\n",
      " [ 0.082271  0.039296]\n",
      " [ 0.008468 -0.066625]\n",
      " [-0.010904 -0.073099]\n",
      " [-0.047584  0.013949]\n",
      " [-0.03296  -0.064206]\n",
      " [-0.019075 -0.08873 ]\n",
      " [ 0.036257  0.002118]\n",
      " [-0.057582 -0.080327]\n",
      " [-0.067484 -0.025774]\n",
      " [ 0.053809 -0.034849]\n",
      " [-0.043297 -0.080974]\n",
      " [-0.078199 -0.096179]\n",
      " [-0.027515 -0.09704 ]\n",
      " [-0.057511 -0.069999]\n",
      " [-0.029968 -0.031442]\n",
      " [ 0.075343 -0.005112]\n",
      " [ 0.028793 -0.037374]\n",
      " [-0.09795  -0.02793 ]\n",
      " [ 0.065122 -0.066848]\n",
      " [ 0.002035  0.05098 ]\n",
      " [-0.050024  0.003156]\n",
      " [ 0.034792 -0.044339]\n",
      " [ 0.065957  0.055399]\n",
      " [ 0.077965 -0.056601]\n",
      " [ 0.082928 -0.029402]\n",
      " [-0.023136  0.007874]\n",
      " [ 0.049568  0.023402]\n",
      " [-0.006345 -0.003922]\n",
      " [-0.031435  0.038307]\n",
      " [ 0.069777 -0.081998]\n",
      " [-0.074564 -0.002265]]\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "key = jax.random.key(4532)\n",
    "keys = jax.random.split(key, controller_batch_size)\n",
    "\n",
    "print(keys.shape)\n",
    "\n",
    "test_data = jit_v_init_data(keys, mjx_model)\n",
    "print(test_data.qpos)\n",
    "print(\"___________\")\n",
    "print(test_data.qvel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowering\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected function with aux output to return a two-element tuple, but got type <class 'jax._src.interpreters.ad.JVPTracer'> with value Traced<ShapedArray(float32[])>with<JVPTrace(level=3/0)> with\n  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=2/0)> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7e2c4bf92bd0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=2/0)>,), out_tracer_refs=[<weakref at 0x7e2d1028e8e0; to 'JaxprTracer' at 0x7e2d1028e610>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[]. let b:f32[] = neg a in (b,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False,), 'name': '_negative', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7e2d13572440>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=False, xla_metadata={}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m controller_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, controller_batch_size)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlowering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m g_loss_lower \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_argnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontroller_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontroller_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmjx_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontroller_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m jit_g_loss \u001b[38;5;241m=\u001b[39m g_loss_lower\u001b[38;5;241m.\u001b[39mcompile()\n",
      "    \u001b[0;31m[... skipping hidden 20 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/biomujoco/lib/python3.11/site-packages/jax/_src/api_util.py:126\u001b[0m, in \u001b[0;36mflatten_fun_nokwargs2\u001b[0;34m(in_tree, *args_flat)\u001b[0m\n\u001b[1;32m    124\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m py_args, {}\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pair, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected function with aux output to return a two-element \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuple, but got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m ans, aux \u001b[38;5;241m=\u001b[39m pair\n\u001b[1;32m    129\u001b[0m ans_flat, ans_tree \u001b[38;5;241m=\u001b[39m tree_flatten(ans)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected function with aux output to return a two-element tuple, but got type <class 'jax._src.interpreters.ad.JVPTracer'> with value Traced<ShapedArray(float32[])>with<JVPTrace(level=3/0)> with\n  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=2/0)> with\n    pval = (ShapedArray(float32[]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7e2c4bf92bd0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=2/0)>,), out_tracer_refs=[<weakref at 0x7e2d1028e8e0; to 'JaxprTracer' at 0x7e2d1028e610>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[]. let b:f32[] = neg a in (b,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False,), 'name': '_negative', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7e2d13572440>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=False, xla_metadata={}))"
     ]
    }
   ],
   "source": [
    "def decay_sum_scan(x, decay):\n",
    "    def f(sxtm1, xt):\n",
    "        b = xt + decay * sxtm1\n",
    "        return b, b\n",
    "    return jax.lax.scan(f, jp.zeros(x.shape[1:]), x)[1]\n",
    "\n",
    "def controller_loss(controller_params, critic_params, batch, batch_size, mjx_model, keys):\n",
    "    # Generate data for simulation\n",
    "    nq = mjx_model.nq\n",
    "    nmtu = mjx_model.nbiomtu\n",
    "    \n",
    "    in_data = batch\n",
    "    \n",
    "    out_data, keys, new_exps = v_nn_multi_steps(controller_params, mjx_model, in_data, keys)\n",
    "    # out_states = new_exps.next_states\n",
    "    # jax.debug.print(\"out_states shape{0}\", out_states.shape)\n",
    "    out_states = jp.concat([out_data.qpos, out_data.qvel, out_data.biomtu.fiber_l, out_data.biomtu.fiber_v, out_data.sensordata],axis=1)\n",
    "    decay_M = 0.99 ** jp.arange(new_exps.rewards.shape[1]-1)\n",
    "    # print(new_exps.rewards.shape)\n",
    "    loss = -jp.mean(decay_M@new_exps.rewards.T[:-1,:])\n",
    "    # loss = -jp.mean(decay_M@new_exps.rewards.T[:-1,:]) - jp.mean(critic_score)\n",
    "    # loss = -jp.mean(critic_score)\n",
    "    return loss\n",
    "\n",
    "# The function calculating the loss of the controller and also generate experiences\n",
    "g_loss = jax.value_and_grad(controller_loss, has_aux=True)\n",
    "\n",
    "controller_keys = jax.random.split(key, controller_batch_size)\n",
    "\n",
    "print(\"lowering\")\n",
    "g_loss_lower = jax.jit(g_loss, static_argnames=[\"batch_size\"]).lower(\n",
    "    controller_params, \n",
    "    critic_params, \n",
    "    test_data, \n",
    "    controller_batch_size, \n",
    "    mjx_model, \n",
    "    controller_keys)\n",
    "\n",
    "print(\"compiling\")\n",
    "jit_g_loss = g_loss_lower.compile()\n",
    "\n",
    "b = jit_g_loss.cost_analysis()[0]['flops']\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 0 -------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Function compiled with input pytree does not match the input pytree it was called with. There are 1 mismatches, including:\n    * at args[2], seen <class 'exp.experience'> but now given <class 'mujoco.mjx._src.types.Data'>, so their Python types differ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------\u001b[39m\u001b[38;5;124m\"\u001b[39m,i,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     controller_loss_exps, controller_loss_grad \u001b[38;5;241m=\u001b[39m \u001b[43mjit_g_loss_experience\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontroller_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmjx_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     controller_loss \u001b[38;5;241m=\u001b[39m controller_loss_exps[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     exps \u001b[38;5;241m=\u001b[39m controller_loss_exps[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/biomujoco/lib/python3.11/site-packages/jax/_src/stages.py:594\u001b[0m, in \u001b[0;36mCompiled.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m outs\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call \u001b[38;5;241m=\u001b[39m cpp_call_fallback\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/biomujoco/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:3052\u001b[0m, in \u001b[0;36mMeshExecutable.create_cpp_call.<locals>.aot_cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maot_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3051\u001b[0m   params \u001b[38;5;241m=\u001b[39m stages\u001b[38;5;241m.\u001b[39mCompiledCallParams(\u001b[38;5;28mself\u001b[39m, no_kwargs, in_tree, out_tree)\n\u001b[0;32m-> 3052\u001b[0m   outs, out_flat, args_flat \u001b[38;5;241m=\u001b[39m \u001b[43mstages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3053\u001b[0m   out_flat, out_tree_dispatch \u001b[38;5;241m=\u001b[39m reflatten_outputs_for_dispatch(\n\u001b[1;32m   3054\u001b[0m       out_tree, out_flat)\n\u001b[1;32m   3055\u001b[0m   use_fastpath \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, xc\u001b[38;5;241m.\u001b[39mArrayImpl) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out_flat))\n",
      "File \u001b[0;32m~/anaconda3/envs/biomujoco/lib/python3.11/site-packages/jax/_src/stages.py:561\u001b[0m, in \u001b[0;36mCompiled.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     base \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m'\u001b[39m][fst\u001b[38;5;241m.\u001b[39midx]\n\u001b[1;32m    558\u001b[0m     msg\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    * at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtree_util\u001b[38;5;241m.\u001b[39mkeystr(\u001b[38;5;28mtuple\u001b[39m(rest))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seen \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthing2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthing1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, so \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 561\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg))\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mexecutable\u001b[38;5;241m.\u001b[39mcall(\u001b[38;5;241m*\u001b[39margs_flat)\n",
      "\u001b[0;31mTypeError\u001b[0m: Function compiled with input pytree does not match the input pytree it was called with. There are 1 mismatches, including:\n    * at args[2], seen <class 'exp.experience'> but now given <class 'mujoco.mjx._src.types.Data'>, so their Python types differ"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(2024)\n",
    "\n",
    "for i in range(500):\n",
    "    key = jax.random.split(key, 1)[0]\n",
    "    keys = jax.random.split(key, controller_batch_size)\n",
    "    \n",
    "    train_data = jit_v_init_data(keys, mjx_model)\n",
    "    \n",
    "    print(\"-----------\",i,\"-------------\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        controller_loss_exps, controller_loss_grad = jit_g_loss(\n",
    "                controller_params, critic_params, train_data, mjx_model, keys)\n",
    "        \n",
    "        controller_loss = controller_loss_exps[0]\n",
    "        exps = controller_loss_exps[1]\n",
    "        \n",
    "        # Update params\n",
    "        controller_updates, controller_opt_state = jit_controller_tx_update(controller_loss_grad, controller_opt_state)\n",
    "        # print(controller_updates)\n",
    "        controller_params = jit_apply_update(controller_params, controller_updates)\n",
    "        \n",
    "        print(\"Controller Loss:\", controller_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "# jax.config.update(\"jax_debug_nans\", False)\n",
    "import exp\n",
    "\n",
    "\n",
    "death_rate_buf = []\n",
    "best_death_rate = 0.5\n",
    "\n",
    "for i in range(4000):\n",
    "\n",
    "    print(\"----------\",i,\"----------\")\n",
    "    # criric_repeat = 1\n",
    "    # if(i >= 200):\n",
    "    #     criric_repeat = 1\n",
    "        \n",
    "    # for j in range(criric_repeat):\n",
    "    #     batch = jit_sample(exp_pool, critic_batch_size, key)\n",
    "    #     key = jax.random.split(key, 1)[0]\n",
    "        \n",
    "    #     # criticer\n",
    "    #     critic_loss, critic_loss_grad = jit_critic_loss_g_value(critic_params, batch)\n",
    "        \n",
    "    #     # Update params\n",
    "    #     critic_updates, critic_opt_state = jit_critic_tx_update(critic_loss_grad, critic_opt_state)\n",
    "    #     critic_params = jit_apply_update(critic_params, critic_updates)\n",
    "    \n",
    "    # print(\"criticer loss:\",critic_loss)\n",
    "    # controller\n",
    "    \n",
    "    controller_repeat = 1\n",
    "    # if(i >= 10):\n",
    "        # controller_repeat = 5\n",
    "    \n",
    "    for j in range(controller_repeat):\n",
    "        keys = jax.random.split(key,controller_batch_size)\n",
    "        controller_batch = jit_sample(exp_pool, controller_batch_size, key)\n",
    "        key = jax.random.split(key, 1)[0]\n",
    "        controller_loss_exps, controller_loss_grad = jit_g_loss_experience(\n",
    "            controller_params, critic_params, controller_batch, mjx_model, init_data_batch, keys)\n",
    "        controller_loss = controller_loss_exps[0]\n",
    "        exps = controller_loss_exps[1]\n",
    "            \n",
    "        # Update params\n",
    "        controller_updates, controller_opt_state = jit_controller_tx_update(controller_loss_grad, controller_opt_state)\n",
    "        # print(controller_updates)\n",
    "        controller_params = jit_apply_update(controller_params, controller_updates)\n",
    "    \n",
    "    # exp_pool = jit_add_exp(memory_settings, exp_pool, exps)\n",
    "    \n",
    "    # Count Dones\n",
    "    death_rate_buf.insert(0, jp.count_nonzero( exps.dones)/(exps.dones.shape[1]*controller_batch_size))\n",
    "    death_rate_buf = death_rate_buf[:50]\n",
    "    death_rate = np.mean(death_rate_buf)\n",
    "    \n",
    "    print(\"Controller Loss:\", controller_loss)\n",
    "    mean_reward = jp.mean(exps.rewards)\n",
    "    print(\"mean rewards:\", mean_reward)\n",
    "    mean_score = jp.mean(jit_v_criticer(critic_params, exps.states))\n",
    "    print(\"mean score:\", mean_score)\n",
    "    print(\"scorc/reward:\", mean_score/mean_reward)\n",
    "    print(\"death rate:\", death_rate)\n",
    "    # print(controller_loss_grad)\n",
    "    \n",
    "    if(best_death_rate > death_rate): best_death_rate = death_rate\n",
    "    # print(\"best_death_rate:\", best_death_rate)\n",
    "    # add exps\n",
    "    \n",
    "    # print(controller_loss_grad)\n",
    "    print(jax.tree.map(jp.max,controller_loss_grad))\n",
    "    # print(controller_params)\n",
    "    print(controller_updates['params']['linear1']['kernel'])\n",
    "    print(controller_updates['params']['linear2']['kernel'])\n",
    "    print(controller_updates['params']['linear3']['kernel'])\n",
    "    print(controller_updates['params']['linear4']['kernel'])\n",
    "    print(controller_updates['params']['linear5']['kernel'])\n",
    "    if(i%20 == 0):\n",
    "        plt.figure()\n",
    "        plt.plot(exp_pool.rewards)\n",
    "        plt.plot()\n",
    "        # plt.plot(exp_pool.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(controller_params['params']['linear1']['kernel'])\n",
    "# print(controller_params['params']['linear2']['kernel'])\n",
    "# print(controller_params['params']['linear3']['kernel'])\n",
    "# print(controller_params['params']['linear4']['kernel'])\n",
    "# print(controller_params['params']['linear5']['kernel'])\n",
    "print(controller_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco.viewer\n",
    "import time\n",
    "\n",
    "\n",
    "mjx_data = mjx.make_data(mjx_model)\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "previous_frame_time = time.time()\n",
    "i = 0\n",
    "key = jax.random.key(334)\n",
    "with mujoco.viewer.launch_passive(mj_model, mj_data) as viewer:\n",
    "    while viewer.is_running():\n",
    "        # Update mjx_data from mj_data. The mj_data was modified by the viewer\n",
    "        # mjx_data = mjx_data.replace(ctrl=mj_data.ctrl, xfrc_applied=mj_data.xfrc_applied)\n",
    "        # Use the nerual network to generate ctrl signal\n",
    "        # Generate key\n",
    "        mjx_data = mjx_data.replace(xfrc_applied=jp.array(mj_data.xfrc_applied, dtype=jp.float32))\n",
    "        mjx_data = mjx_data.replace(\n",
    "            qpos= jp.array(mj_data.qpos, dtype=jp.float32),\n",
    "            qvel= jp.array(mj_data.qvel, dtype=jp.float32),\n",
    "            time = jp.array(mj_data.time, dtype=jp.float32))\n",
    "        \n",
    "        # Update mjx_model from mj_model\n",
    "        mjx_model = mjx_model.tree_replace({\n",
    "            'opt.gravity': jp.array(mj_model.opt.gravity, dtype=jp.float32),\n",
    "            'opt.tolerance': jp.array(mj_model.opt.tolerance, dtype=jp.float32),\n",
    "            'opt.ls_tolerance': jp.array(mj_model.opt.ls_tolerance, dtype=jp.float32),\n",
    "            'opt.timestep': jp.array(mj_model.opt.timestep, dtype=jp.float32),\n",
    "        })\n",
    "        \n",
    "        key = jax.random.split(key,1)[0]\n",
    "        # mjx_data = mjx_step(mjx_model, mjx_data)\n",
    "        # mjx_data, loss, expp = jit_nn_multi_steps(controller_params, mjx_model, mjx_data, key)\n",
    "        mjx_data, key, act = jit_nn_mjx_one_step_no_random(controller_params, mjx_model, mjx_data, key)\n",
    "        mjx.get_data_into(mj_data, mj_model, mjx_data)\n",
    "        \n",
    "        # Record the current time at the start of this frame\n",
    "        current_frame_time = time.time()\n",
    "    \n",
    "        # Calculate the difference in time from the last frame\n",
    "        time_between_frames = current_frame_time - previous_frame_time\n",
    "    \n",
    "        # Print the time between frames\n",
    "        print(f\"Time between frames: {time_between_frames} seconds\")\n",
    "        previous_frame_time = current_frame_time\n",
    "        \n",
    "        print(\"ACT:\", mjx_data.biomtu.act)\n",
    "        print(mjx_data.qpos)\n",
    "        # print(mjx_data.sensordata)\n",
    "        # print(len(mjx_data.qvel))\n",
    "        viewer.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jp\n",
    "0.99 ** jp.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jp\n",
    "\n",
    "def test_fn(x,carry):\n",
    "    x0 = carry\n",
    "    carry = x\n",
    "    return x0+x, carry\n",
    "\n",
    "xs = jp.linspace(0,10,11)\n",
    "\n",
    "x, carry = jax.lax.scan(test_fn, 0, xs)\n",
    "\n",
    "print(x,carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
